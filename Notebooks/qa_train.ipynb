{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path = './raw_data/ir_data/privacy_policy/policy_train_data.csv',\n",
    "    pretuned_model_path = 'bert-base-uncased',\n",
    "    model_save_path='./models/qa_model',\n",
    "    num_samples=15000,\n",
    "    batch_size = 16,\n",
    "    learn_rate = 2e-5,\n",
    "    epochs = 5,\n",
    "    device='cpu',\n",
    "    train_split=0.7,\n",
    "    patience = 3,\n",
    "    freeze=False\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('casehold/legalbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.data_path, sep='\\t')[:args.num_samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>DocID</th>\n",
       "      <th>QueryID</th>\n",
       "      <th>SentID</th>\n",
       "      <th>Split</th>\n",
       "      <th>Query</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_0</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>This privacy policy, with our Terms of Servic...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_1</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>We encourage you to read this privacy policy c...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_2</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>By using our application or other online servi...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_3</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>When we post changes to this privacy policy, ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_4</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>We encourage you to review this privacy policy...</td>\n",
       "      <td>Irrelevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Folder                DocID  \\\n",
       "0  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "1  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "2  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "3  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "4  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "\n",
       "                 QueryID                   SentID  Split  \\\n",
       "0  Cake Web Browser _1_0  Cake Web Browser _1_0_0  train   \n",
       "1  Cake Web Browser _1_0  Cake Web Browser _1_0_1  train   \n",
       "2  Cake Web Browser _1_0  Cake Web Browser _1_0_2  train   \n",
       "3  Cake Web Browser _1_0  Cake Web Browser _1_0_3  train   \n",
       "4  Cake Web Browser _1_0  Cake Web Browser _1_0_4  train   \n",
       "\n",
       "                                               Query  \\\n",
       "0  do you keep the data of mine and upload to you...   \n",
       "1  do you keep the data of mine and upload to you...   \n",
       "2  do you keep the data of mine and upload to you...   \n",
       "3  do you keep the data of mine and upload to you...   \n",
       "4  do you keep the data of mine and upload to you...   \n",
       "\n",
       "                                             Segment       Label  \n",
       "0   This privacy policy, with our Terms of Servic...  Irrelevant  \n",
       "1  We encourage you to read this privacy policy c...  Irrelevant  \n",
       "2  By using our application or other online servi...  Irrelevant  \n",
       "3   When we post changes to this privacy policy, ...  Irrelevant  \n",
       "4  We encourage you to review this privacy policy...  Irrelevant  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of train samples : 10501\n",
      "Number of val samples : 2249\n",
      "Number of test samples : 2250\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>DocID</th>\n",
       "      <th>QueryID</th>\n",
       "      <th>SentID</th>\n",
       "      <th>Split</th>\n",
       "      <th>Query</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_0</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>This privacy policy, with our Terms of Servic...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_1</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>We encourage you to read this privacy policy c...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_2</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>By using our application or other online servi...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_3</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>When we post changes to this privacy policy, ...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_4</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>We encourage you to review this privacy policy...</td>\n",
       "      <td>Irrelevant</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Folder                DocID  \\\n",
       "0  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "1  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "2  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "3  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "4  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "\n",
       "                 QueryID                   SentID  Split  \\\n",
       "0  Cake Web Browser _1_0  Cake Web Browser _1_0_0  train   \n",
       "1  Cake Web Browser _1_0  Cake Web Browser _1_0_1  train   \n",
       "2  Cake Web Browser _1_0  Cake Web Browser _1_0_2  train   \n",
       "3  Cake Web Browser _1_0  Cake Web Browser _1_0_3  train   \n",
       "4  Cake Web Browser _1_0  Cake Web Browser _1_0_4  train   \n",
       "\n",
       "                                               Query  \\\n",
       "0  do you keep the data of mine and upload to you...   \n",
       "1  do you keep the data of mine and upload to you...   \n",
       "2  do you keep the data of mine and upload to you...   \n",
       "3  do you keep the data of mine and upload to you...   \n",
       "4  do you keep the data of mine and upload to you...   \n",
       "\n",
       "                                             Segment       Label split  \n",
       "0   This privacy policy, with our Terms of Servic...  Irrelevant   val  \n",
       "1  We encourage you to read this privacy policy c...  Irrelevant   val  \n",
       "2  By using our application or other online servi...  Irrelevant   val  \n",
       "3   When we post changes to this privacy policy, ...  Irrelevant   val  \n",
       "4  We encourage you to review this privacy policy...  Irrelevant   val  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['split'] = 'train'\n",
    "\n",
    "num_val_rows = int(len(df) * (1 - args.train_split)//2) - 1\n",
    "\n",
    "# 15% for validation and test each , remaining 70% for train\n",
    "df.loc[:num_val_rows, 'split'] = 'val'\n",
    "df.loc[num_val_rows: num_val_rows + num_val_rows, 'split'] = 'test'\n",
    "\n",
    "print('Number of train samples : ' + str((df['split'] == 'train').sum()))\n",
    "print('Number of val samples : ' + str((df['split'] == 'val').sum()))\n",
    "print('Number of test samples : ' + str((df['split'] == 'test').sum()))\n",
    "\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Folder</th>\n",
       "      <th>DocID</th>\n",
       "      <th>QueryID</th>\n",
       "      <th>SentID</th>\n",
       "      <th>Split</th>\n",
       "      <th>Query</th>\n",
       "      <th>Segment</th>\n",
       "      <th>Label</th>\n",
       "      <th>split</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_0</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>This privacy policy, with our Terms of Servic...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_1</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>We encourage you to read this privacy policy c...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_2</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>By using our application or other online servi...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_3</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>When we post changes to this privacy policy, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>../../Dataset/Train/com.cake.browser</td>\n",
       "      <td>Cake Web Browser _1</td>\n",
       "      <td>Cake Web Browser _1_0</td>\n",
       "      <td>Cake Web Browser _1_0_4</td>\n",
       "      <td>train</td>\n",
       "      <td>do you keep the data of mine and upload to you...</td>\n",
       "      <td>We encourage you to review this privacy policy...</td>\n",
       "      <td>0</td>\n",
       "      <td>val</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                 Folder                DocID  \\\n",
       "0  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "1  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "2  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "3  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "4  ../../Dataset/Train/com.cake.browser  Cake Web Browser _1   \n",
       "\n",
       "                 QueryID                   SentID  Split  \\\n",
       "0  Cake Web Browser _1_0  Cake Web Browser _1_0_0  train   \n",
       "1  Cake Web Browser _1_0  Cake Web Browser _1_0_1  train   \n",
       "2  Cake Web Browser _1_0  Cake Web Browser _1_0_2  train   \n",
       "3  Cake Web Browser _1_0  Cake Web Browser _1_0_3  train   \n",
       "4  Cake Web Browser _1_0  Cake Web Browser _1_0_4  train   \n",
       "\n",
       "                                               Query  \\\n",
       "0  do you keep the data of mine and upload to you...   \n",
       "1  do you keep the data of mine and upload to you...   \n",
       "2  do you keep the data of mine and upload to you...   \n",
       "3  do you keep the data of mine and upload to you...   \n",
       "4  do you keep the data of mine and upload to you...   \n",
       "\n",
       "                                             Segment  Label split  \n",
       "0   This privacy policy, with our Terms of Servic...      0   val  \n",
       "1  We encourage you to read this privacy policy c...      0   val  \n",
       "2  By using our application or other online servi...      0   val  \n",
       "3   When we post changes to this privacy policy, ...      0   val  \n",
       "4  We encourage you to review this privacy policy...      0   val  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapping = {'Irrelevant' : 0, 'Relevant' : 1}\n",
    "df['Label'] = df['Label'].map(mapping)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############## Setup Train Dataloader ##############\n",
    "####################################################\n",
    "\n",
    "encoded_data_train = [tokenizer.encode_plus(row['Query'], row['Segment'], add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True) for index,row in df[df['split'] == 'train'].iterrows()]\n",
    "input_ids_train = [item['input_ids'] for item in encoded_data_train]\n",
    "attention_masks_train = [item['attention_mask'] for item in encoded_data_train]\n",
    "labels_train = [row['Label'] for index,row in df[df['split'] == 'train'].iterrows()]\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_train = torch.tensor(input_ids_train)\n",
    "attention_masks_train = torch.tensor(attention_masks_train)\n",
    "labels_train = torch.tensor(labels_train)\n",
    "\n",
    "# Create a dataset\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=args.batch_size, shuffle=True) # NOTE : maybe set pin_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############## Setup Val Dataloader ################\n",
    "####################################################\n",
    "\n",
    "encoded_data_val = [tokenizer.encode_plus(row['Query'], row['Segment'], add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True) for index,row in df[df['split'] == 'val'].iterrows()]\n",
    "input_ids_val = [item['input_ids'] for item in encoded_data_val]\n",
    "attention_masks_val = [item['attention_mask'] for item in encoded_data_val]\n",
    "labels_val = [row['Label'] for index,row in df[df['split'] == 'val'].iterrows()]\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_val = torch.tensor(input_ids_val)\n",
    "attention_masks_val = torch.tensor(attention_masks_val)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "# Create a dataset\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=args.batch_size, shuffle=True) # NOTE : maybe set pin_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "###################################################\n",
    "############## Setup Test Dataloader ##############\n",
    "###################################################\n",
    "\n",
    "encoded_data_test = [tokenizer.encode_plus(row['Query'], row['Segment'], add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True) for index,row in df[df['split'] == 'test'].iterrows()]\n",
    "input_ids_test = [item['input_ids'] for item in encoded_data_test]\n",
    "attention_masks_test = [item['attention_mask'] for item in encoded_data_test]\n",
    "labels_test = [row['Label'] for index,row in df[df['split'] == 'test'].iterrows()]\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_test = torch.tensor(input_ids_test)\n",
    "attention_masks_test = torch.tensor(attention_masks_test)\n",
    "labels_test = torch.tensor(labels_test)\n",
    "\n",
    "# Create a dataset\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=args.batch_size, shuffle=True) # NOTE : maybe set pin_memory=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "\n",
    "def calculate_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Initialize model and optimizer\n",
    "model = BertForSequenceClassification.from_pretrained(args.pretuned_model_path)\n",
    "\n",
    "if args.freeze: \n",
    "    for param in model.base_model.parameters():\n",
    "        param.requires_grad  = False\n",
    "\n",
    "\n",
    "if args.freeze: \n",
    "    optimizer = AdamW(model.classifier.parameters(), lr=args.learn_rate)\n",
    "else : \n",
    "    optimizer = AdamW(model.parameters(), lr=args.learn_rate)\n",
    "\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(dataloader_train) * 0.05, num_training_steps=len(dataloader_train) * args.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "torch.cuda.empty_cache()\n",
    "if torch.cuda.is_available():\n",
    "  args.device = 'cuda'\n",
    "\n",
    "model.to(args.device)\n",
    "print(args.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a6edcba6cc1643e09aa817c300336c6e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cd29e2bab5fa4d17bf8e8567feb9c1b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea6b95af4a084b76a7a94acf4209cf42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average Training Loss: 0.16887832330072036\n",
      "Epoch 0: Validation Accuracy: 0.9490248226950354\n",
      "Validation accuracy improved from 0.0 to 0.9490248226950354. Saving model...\n",
      "Epoch 1: Average Training Loss: 0.08187438722951848\n",
      "Epoch 1: Validation Accuracy: 0.9535559495665878\n",
      "Validation accuracy improved from 0.9490248226950354 to 0.9535559495665878. Saving model...\n",
      "Epoch 2: Average Training Loss: 0.0449700141061355\n",
      "Epoch 2: Validation Accuracy: 0.9583333333333334\n",
      "Validation accuracy improved from 0.9535559495665878 to 0.9583333333333334. Saving model...\n",
      "Epoch 3: Average Training Loss: 0.016689012151450622\n",
      "Epoch 3: Validation Accuracy: 0.9605496453900709\n",
      "Validation accuracy improved from 0.9583333333333334 to 0.9605496453900709. Saving model...\n",
      "Epoch 4: Average Training Loss: 0.0064789490723486136\n",
      "Epoch 4: Validation Accuracy: 0.9547872340425532\n"
     ]
    }
   ],
   "source": [
    "train_progress = tqdm(total=0, desc='Train Batches', leave=True)\n",
    "validation_progress = tqdm(total=0, desc='Validation Batches', leave=True)\n",
    "epoch_progress = tqdm(total=args.epochs, desc='Epoch', leave=True)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "patience = 3\n",
    "num_epochs_no_improvement = 0\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  total_train_accuracy = 0 #NEW\n",
    "\n",
    "  train_progress.reset(total=len(dataloader_train))\n",
    "  validation_progress.reset(total=len(dataloader_val))\n",
    "\n",
    "  # Training Loop\n",
    "  for step, batch in enumerate(dataloader_train):\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask, b_labels = b_input_ids.to(args.device), b_input_mask.to(args.device), b_labels.to(args.device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    total_train_loss += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()#NEW\n",
    "    label_ids = b_labels.to('cpu').numpy()#NEW\n",
    "    total_train_accuracy += calculate_accuracy(logits, label_ids)#NEW\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    train_progress.update(1)\n",
    "\n",
    "  avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "  print(f'Epoch {epoch}: Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "  model.eval()\n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "\n",
    "  # Validation Loop\n",
    "  for batch in dataloader_val:\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask, b_labels = b_input_ids.to(args.device), b_input_mask.to(args.device), b_labels.to(args.device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    total_eval_loss += loss_fn(outputs.logits.squeeze(-1), b_labels).item() # perhaps just outputs.loss (need to include labels as parameter in model() above)\n",
    "\n",
    "    total_eval_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    validation_progress.update(1)\n",
    "\n",
    "  avg_val_accuracy = total_eval_accuracy / len(dataloader_val)\n",
    "  print(f'Epoch {epoch}: Validation Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "  # Checkpointing and Early Stopping\n",
    "  if avg_val_accuracy > best_val_accuracy:\n",
    "      print(f'Validation accuracy improved from {best_val_accuracy} to {avg_val_accuracy}. Saving model...')\n",
    "      best_val_accuracy = avg_val_accuracy\n",
    "      num_epochs_no_improvement = 0\n",
    "      # Save the model using save_pretrained\n",
    "      model.save_pretrained(args.model_save_path)\n",
    "  else:\n",
    "      num_epochs_no_improvement += 1\n",
    "      if num_epochs_no_improvement >= args.patience:\n",
    "          print(\"Early stopping triggered.\")\n",
    "          break  # Exit the training loop\n",
    "\n",
    "  epoch_progress.update(1)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "def evaluate_sequence_pair_class(model_path,  title):\n",
    "    '''\n",
    "    Routine for evaluating model for sequence pair classification\n",
    "    '''\n",
    "\n",
    "    progress = tqdm(total=len(dataloader_test), desc='Train Batches', leave=True)\n",
    "\n",
    "    # load model and tokenizer\n",
    "    # model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path, num_labels=2)\n",
    "\n",
    "\n",
    "    # Check if cuda available\n",
    "    if torch.cuda.is_available():\n",
    "        # model.to('cuda')\n",
    "        args.device = 'cuda'\n",
    "    else:\n",
    "        # model.to('cpu')\n",
    "        args.device = 'cpu'\n",
    "\n",
    "    print(args.device)\n",
    "\n",
    "    model.to(args.device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    print('Evaluating ' + f'[{title}]')\n",
    "    print('============================================')\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # disable calculating gradients (more efficient for evaluation)\n",
    "        for batch in dataloader_test:\n",
    "            progress.update(1)\n",
    "\n",
    "            input_ids, attention_mask, labels = tuple(t.to(args.device) for t in batch)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).flatten() # find index of max value in logits tensor (where each index corresponds to a binary class)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, acc = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    print(f'Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6ba3d7b2f5b44ef294c4421441f600bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "2\n",
      "2\n",
      "None\n",
      "Accuracy: 0.9742222222222222\n",
      "Precision: 0.696969696969697\n",
      "Recall: 0.323943661971831\n",
      "F1 Score: 0.4423076923076923\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/qa_model',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fa34a903f3764d568102a353a9d61492",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "Accuracy: 0.9582222222222222\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/sentence_pair_classification',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
