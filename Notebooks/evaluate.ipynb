{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import matplotlib.pyplot as plt\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "from transformers import BertTokenizer, DataCollatorForLanguageModeling\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset, Dataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    data_path = './processed_data/combined_ir.csv',\n",
    "    base_model_path='./models/parallel_three_mlm/',\n",
    "    model_save_path = './models/test_combined_parallel_three_mlm',\n",
    "    max_samples=15000,\n",
    "    train_split=0.7,\n",
    "    epochs = 5\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pre-process Data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(args.data_path)[0:args.max_samples]\n",
    "num_train_rows = int(len(df) * (1 - args.train_split)//2) - 1\n",
    "df['split'] = 'train'\n",
    "df.loc[:num_train_rows, 'split'] = 'val'\n",
    "df.loc[num_train_rows:num_train_rows + num_train_rows, 'split'] = 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Train Split      : 10501\n",
      "Length of Train Validation : 2249\n",
      "Length of Train Test       : 2250\n"
     ]
    }
   ],
   "source": [
    "print('Length of Train Split      :', len(df[df['split'] == 'train']))\n",
    "print('Length of Train Validation :', len(df[df['split'] == 'val']))\n",
    "print('Length of Train Test       :', len(df[df['split'] == 'test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('casehold/legalbert')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/transformers/tokenization_utils_base.py:2619: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
      "  warnings.warn(\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############## Setup Train Dataloader ##############\n",
    "####################################################\n",
    "\n",
    "encoded_data_train = [tokenizer.encode_plus(row['query'], row['result'], add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True) for index,row in df[df['split'] == 'train'].iterrows()]\n",
    "input_ids_train = [item['input_ids'] for item in encoded_data_train]\n",
    "attention_masks_train = [item['attention_mask'] for item in encoded_data_train]\n",
    "labels_train = [row['label'] for index,row in df[df['split'] == 'train'].iterrows()]\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_train = torch.tensor(input_ids_train)\n",
    "attention_masks_train = torch.tensor(attention_masks_train)\n",
    "labels_train = torch.tensor(labels_train)\n",
    "\n",
    "\n",
    "dataset_train = TensorDataset(input_ids_train, attention_masks_train, labels_train)\n",
    "\n",
    "dataloader_train = DataLoader(dataset_train, batch_size=16, shuffle=True) # NOTE : maybe set pin_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "####################################################\n",
    "############## Setup Val Dataloader ################\n",
    "####################################################\n",
    "\n",
    "encoded_data_val = [tokenizer.encode_plus(row['query'], row['result'], add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True) for index,row in df[df['split'] == 'val'].iterrows()]\n",
    "input_ids_val = [item['input_ids'] for item in encoded_data_val]\n",
    "attention_masks_val = [item['attention_mask'] for item in encoded_data_val]\n",
    "labels_val = [row['label'] for index,row in df[df['split'] == 'val'].iterrows()]\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_val = torch.tensor(input_ids_val)\n",
    "attention_masks_val = torch.tensor(attention_masks_val)\n",
    "labels_val = torch.tensor(labels_val)\n",
    "\n",
    "# Create a dataset\n",
    "dataset_val = TensorDataset(input_ids_val, attention_masks_val, labels_val)\n",
    "\n",
    "dataloader_val = DataLoader(dataset_val, batch_size=16, shuffle=True) # NOTE : maybe set pin_memory=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n",
      "Be aware, overflowing tokens are not returned for the setting you have chosen, i.e. sequence pairs with the 'longest_first' truncation strategy. So the returned list will always be empty even if some tokens have been removed.\n"
     ]
    }
   ],
   "source": [
    "###################################################\n",
    "############## Setup Test Dataloader ##############\n",
    "###################################################\n",
    "\n",
    "encoded_data_test = [tokenizer.encode_plus(row['query'], row['result'], add_special_tokens=True, max_length=512, pad_to_max_length=True, truncation=True) for index,row in df[df['split'] == 'test'].iterrows()]\n",
    "input_ids_test = [item['input_ids'] for item in encoded_data_test]\n",
    "attention_masks_test = [item['attention_mask'] for item in encoded_data_test]\n",
    "labels_test = [row['label'] for index,row in df[df['split'] == 'test'].iterrows()]\n",
    "\n",
    "# Convert to tensors\n",
    "input_ids_test = torch.tensor(input_ids_test)\n",
    "attention_masks_test = torch.tensor(attention_masks_test)\n",
    "labels_test = torch.tensor(labels_test)\n",
    "\n",
    "# Create a dataset\n",
    "dataset_test = TensorDataset(input_ids_test, attention_masks_test, labels_test)\n",
    "\n",
    "dataloader_test = DataLoader(dataset_test, batch_size=16, shuffle=True) # NOTE : maybe set pin_memory=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#########################################################\n",
    "############ Train Routine Utility Functions ############\n",
    "#########################################################\n",
    "\n",
    "def compute_accuracy(y_pred, y_target):\n",
    "    y_target = y_target.cpu()\n",
    "    y_pred_indices = (torch.sigmoid(y_pred)>0.5).cpu().long()#.max(dim=1)[1]\n",
    "    n_correct = torch.eq(y_pred_indices, y_target).sum().item()\n",
    "    return n_correct / len(y_pred_indices) * 100\n",
    "\n",
    "\n",
    "def calculate_accuracy(preds, labels):\n",
    "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
    "    labels_flat = labels.flatten()\n",
    "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The fine-tune routine "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ad696a4fefb64ed4bdb5d745905199d8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "99c3d1838ed44a51b48fd245bbd51e3a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e13b978b001d457b998ac9e1abc1a650",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average Training Loss: 0.2929984949801338\n",
      "Epoch 0: Validation Accuracy: 0.9008077226162332\n",
      "Validation accuracy improved from 0.0 to 0.9008077226162332. Saving model...\n",
      "Epoch 1: Average Training Loss: 0.25433447417240346\n",
      "Epoch 1: Validation Accuracy: 0.9111012608353033\n",
      "Validation accuracy improved from 0.9008077226162332 to 0.9111012608353033. Saving model...\n",
      "Epoch 2: Average Training Loss: 0.21988824739076956\n",
      "Epoch 2: Validation Accuracy: 0.9128743104806935\n",
      "Validation accuracy improved from 0.9111012608353033 to 0.9128743104806935. Saving model...\n",
      "Epoch 3: Average Training Loss: 0.15452558021342858\n",
      "Epoch 3: Validation Accuracy: 0.9084416863672182\n",
      "Epoch 4: Average Training Loss: 0.08929925356230371\n",
      "Epoch 4: Validation Accuracy: 0.9060283687943262\n"
     ]
    }
   ],
   "source": [
    "from transformers import AdamW\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(args.base_model_path)\n",
    "\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(dataloader_train) * 0.05, num_training_steps=len(dataloader_train) * args.epochs)\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "\n",
    "print('============================================')\n",
    "\n",
    "\n",
    "train_progress = tqdm(total=0, desc='Train Batches', leave=True)\n",
    "validation_progress = tqdm(total=0, desc='Validation Batches', leave=True)\n",
    "epoch_progress = tqdm(total=args.epochs, desc='Epoch', leave=True)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "patience = 3\n",
    "num_epochs_no_improvement = 0\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  total_train_accuracy = 0 #NEW\n",
    "\n",
    "  train_progress.reset(total=len(dataloader_train))\n",
    "  validation_progress.reset(total=len(dataloader_val))\n",
    "\n",
    "  for step, batch in enumerate(dataloader_train):\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    total_train_loss += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()#NEW\n",
    "    label_ids = b_labels.to('cpu').numpy()#NEW\n",
    "    total_train_accuracy += calculate_accuracy(logits, label_ids)#NEW\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    train_progress.update(1)\n",
    "\n",
    "  avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "  print(f'Epoch {epoch}: Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "  model.eval()\n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "\n",
    "  for batch in dataloader_val:\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    total_eval_loss += loss_fn(outputs.logits.squeeze(-1), b_labels).item() # perhaps just outputs.loss (need to include labels as parameter in model() above)\n",
    "\n",
    "    total_eval_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    validation_progress.update(1)\n",
    "\n",
    "  avg_val_accuracy = total_eval_accuracy / len(dataloader_val)\n",
    "  print(f'Epoch {epoch}: Validation Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "  # Checkpointing and Early Stopping\n",
    "  if avg_val_accuracy > best_val_accuracy:\n",
    "      print(f'Validation accuracy improved from {best_val_accuracy} to {avg_val_accuracy}. Saving model...')\n",
    "      best_val_accuracy = avg_val_accuracy\n",
    "      num_epochs_no_improvement = 0\n",
    "      # Save the model using save_pretrained\n",
    "      model.save_pretrained(args.model_save_path)\n",
    "      # Optionally save the tokenizer if it's being fine-tuned or used\n",
    "      # tokenizer.save_pretrained(checkpoint_path)\n",
    "  else:\n",
    "      num_epochs_no_improvement += 1\n",
    "      if num_epochs_no_improvement >= patience:\n",
    "          print(\"Early stopping triggered.\")\n",
    "          break  # Exit the training loop\n",
    "\n",
    "  epoch_progress.update(1)\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./models/parallel_three_mlm/ and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "439076352\n",
      "494927872\n",
      "============================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/transformers/optimization.py:429: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "403de2c78bc94a5f9915a8f393cc0c5a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Train Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "23acb92dd59648769652881662986d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Validation Batches: 0it [00:00, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8c69455a79945e1a35e2e869832f04c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: Average Training Loss: 0.5500936298609869\n",
      "Epoch 0: Validation Accuracy: 0.818360914105595\n",
      "Epoch 1: Average Training Loss: 0.4761082611898672\n",
      "Epoch 1: Validation Accuracy: 0.8445133963750985\n",
      "Epoch 2: Average Training Loss: 0.45037874790359306\n",
      "Epoch 2: Validation Accuracy: 0.8495862884160756\n",
      "Epoch 3: Average Training Loss: 0.43850378286167185\n",
      "Epoch 3: Validation Accuracy: 0.8551516942474389\n",
      "Epoch 4: Average Training Loss: 0.43475211671407543\n",
      "Epoch 4: Validation Accuracy: 0.8573680063041765\n"
     ]
    }
   ],
   "source": [
    "# FREEZE CORE LAYERS \n",
    "\n",
    "from transformers import AdamW\n",
    "\n",
    "device = 'cpu'\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "model = BertForSequenceClassification.from_pretrained(args.base_model_path)\n",
    "\n",
    "for param in model.base_model.parameters():\n",
    "  param.requires_grad  = False\n",
    "model.classifier = torch.nn.Linear(model.config.hidden_size, 2)\n",
    "\n",
    "model.to(device)\n",
    "model.train()\n",
    "optimizer = AdamW(model.classifier.parameters(), lr=5e-5)\n",
    "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=len(dataloader_train) * 0.05, num_training_steps=len(dataloader_train) * args.epochs)\n",
    "print(torch.cuda.memory_allocated())\n",
    "print(torch.cuda.memory_reserved())\n",
    "\n",
    "print('============================================')\n",
    "\n",
    "train_progress = tqdm(total=0, desc='Train Batches', leave=True)\n",
    "validation_progress = tqdm(total=0, desc='Validation Batches', leave=True)\n",
    "epoch_progress = tqdm(total=args.epochs, desc='Epoch', leave=True)\n",
    "\n",
    "best_val_accuracy = 0.0\n",
    "patience = 3\n",
    "num_epochs_no_improvement = 0\n",
    "\n",
    "\n",
    "loss_fn = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "for epoch in range(args.epochs):\n",
    "\n",
    "  model.train()\n",
    "  total_train_loss = 0\n",
    "  total_train_accuracy = 0 #NEW\n",
    "\n",
    "  train_progress.reset(total=len(dataloader_train))\n",
    "  validation_progress.reset(total=len(dataloader_val))\n",
    "\n",
    "  for step, batch in enumerate(dataloader_train):\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "    model.zero_grad()\n",
    "    outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask, labels=b_labels)\n",
    "\n",
    "    loss = outputs.loss\n",
    "    total_train_loss += loss.item()\n",
    "    loss.backward()\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()#NEW\n",
    "    label_ids = b_labels.to('cpu').numpy()#NEW\n",
    "    total_train_accuracy += calculate_accuracy(logits, label_ids)#NEW\n",
    "\n",
    "    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "    optimizer.step()\n",
    "    scheduler.step()\n",
    "\n",
    "    train_progress.update(1)\n",
    "\n",
    "  avg_train_loss = total_train_loss / len(dataloader_train)\n",
    "  print(f'Epoch {epoch}: Average Training Loss: {avg_train_loss}')\n",
    "\n",
    "  model.eval()\n",
    "  total_eval_accuracy = 0\n",
    "  total_eval_loss = 0\n",
    "\n",
    "  for batch in dataloader_val:\n",
    "    b_input_ids, b_input_mask, b_labels = batch\n",
    "    b_input_ids, b_input_mask, b_labels = b_input_ids.to(device), b_input_mask.to(device), b_labels.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "      outputs = model(b_input_ids, token_type_ids=None, attention_mask=b_input_mask)\n",
    "\n",
    "    logits = outputs.logits.detach().cpu().numpy()\n",
    "    label_ids = b_labels.to('cpu').numpy()\n",
    "    total_eval_loss += loss_fn(outputs.logits.squeeze(-1), b_labels).item() # perhaps just outputs.loss (need to include labels as parameter in model() above)\n",
    "\n",
    "    total_eval_accuracy += calculate_accuracy(logits, label_ids)\n",
    "\n",
    "    validation_progress.update(1)\n",
    "\n",
    "  avg_val_accuracy = total_eval_accuracy / len(dataloader_val)\n",
    "  print(f'Epoch {epoch}: Validation Accuracy: {avg_val_accuracy}')\n",
    "\n",
    "#   # Checkpointing and Early Stopping\n",
    "#   if avg_val_accuracy > best_val_accuracy:\n",
    "#       print(f'Validation accuracy improved from {best_val_accuracy} to {avg_val_accuracy}. Saving model...')\n",
    "#       best_val_accuracy = avg_val_accuracy\n",
    "#       num_epochs_no_improvement = 0\n",
    "#       # Save the model using save_pretrained\n",
    "#       model.save_pretrained(args.model_save_path)\n",
    "#       # Optionally save the tokenizer if it's being fine-tuned or used\n",
    "#       # tokenizer.save_pretrained(checkpoint_path)\n",
    "#   else:\n",
    "#       num_epochs_no_improvement += 1\n",
    "#       if num_epochs_no_improvement >= patience:\n",
    "#           print(\"Early stopping triggered.\")\n",
    "#           break  # Exit the training loop\n",
    "\n",
    "  epoch_progress.update(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(args.model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertTokenizer, BertModel, BertConfig\n",
    "def evaluate_sequence_pair_class(model_path,  title):\n",
    "    '''\n",
    "    Routine for evaluating model for sequence pair classification\n",
    "    '''\n",
    "    batch_progress = tqdm(total=len(dataloader_test), desc='Batches', leave=True)\n",
    "\n",
    "    # load model and tokenizer\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(model_path)\n",
    "    # config = BertConfig.from_pretrained('./models/mlm_model')\n",
    "    # config.num_labels = 2\n",
    "    # model = BertForSequenceClassification(config=config)\n",
    "    # bert_state_dict = torch.load(model_path)\n",
    "    # model.bert.load_state_dict(bert_state_dict)\n",
    "\n",
    "    device = 'cpu'\n",
    "\n",
    "    # Check if cuda available\n",
    "    if torch.cuda.is_available():\n",
    "        # model.to('cuda')\n",
    "        device = 'cuda'\n",
    "\n",
    "    print(device)\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    predictions, true_labels = [], []\n",
    "\n",
    "    print('Evaluating ' + f'[{title}]')\n",
    "    print('============================================')\n",
    "\n",
    "\n",
    "    with torch.no_grad(): # disable calculating gradients (more efficient for evaluation)\n",
    "        for batch in dataloader_test:\n",
    "            input_ids, attention_mask, labels = tuple(t.to(device) for t in batch)\n",
    "            outputs = model(input_ids, attention_mask=attention_mask)\n",
    "            logits = outputs.logits\n",
    "            preds = torch.argmax(logits, dim=1).flatten() # find index of max value in logits tensor (where each index corresponds to a binary class)\n",
    "\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            batch_progress.update(1)\n",
    "\n",
    "\n",
    "    # Calculate metrics\n",
    "    accuracy = accuracy_score(true_labels, predictions)\n",
    "    precision, recall, f1, acc = precision_recall_fscore_support(true_labels, predictions, average='binary')\n",
    "    # roc_auc = roc_auc_score(true_labels, predictions)  # Uncomment if ROC-AUC is needed\n",
    "    print(acc)\n",
    "    print(f'Accuracy: {accuracy}\\nPrecision: {precision}\\nRecall: {recall}\\nF1 Score: {f1}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfa23fe21b634a2f8ccb890126266073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.8857777777777778\n",
      "Precision: 0.9878260869565217\n",
      "Recall: 0.6943765281173594\n",
      "F1 Score: 0.8155061019382628\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/test_combined_mlm',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7df9fdb654334690b27079d452b1c9da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.8395555555555556\n",
      "Precision: 0.8278335724533716\n",
      "Recall: 0.7053789731051344\n",
      "F1 Score: 0.7617161716171618\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/test_combined_base',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "992db9fe4f9d4bab984493607d7baaed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.8515555555555555\n",
      "Precision: 0.9201388888888888\n",
      "Recall: 0.6479217603911981\n",
      "F1 Score: 0.7604017216642754\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/test_combined_parallel_three',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8782c512c9fc452c86bec7d80130958a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.8471111111111111\n",
      "Precision: 0.8885245901639345\n",
      "Recall: 0.6625916870415648\n",
      "F1 Score: 0.7591036414565826\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/test_combined_parallel_three_mlm',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ----------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "756dfa3dcb04478d861612784620903d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.9022222222222223\n",
      "Precision: 0.9657320872274143\n",
      "Recall: 0.7579462102689487\n",
      "F1 Score: 0.8493150684931506\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/parallel_combined',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a244728a0aba45b1a685dee679522786",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.9053333333333333\n",
      "Precision: 0.9886914378029079\n",
      "Recall: 0.7481662591687042\n",
      "F1 Score: 0.8517745302713987\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/parallel_combined_mlm',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5393608fb28742fa90d203bdad4b53d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.9075555555555556\n",
      "Precision: 0.9856687898089171\n",
      "Recall: 0.7567237163814181\n",
      "F1 Score: 0.8561549100968188\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/mlm_combined',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "464b483963624a70b438d45cdfcab268",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.9062222222222223\n",
      "Precision: 0.9918962722852512\n",
      "Recall: 0.7481662591687042\n",
      "F1 Score: 0.8529616724738676\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/basebert_combined',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2396d91264f647628bfe1a548fb1f514",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at casehold/legalbert and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.3325\n",
      "Precision: 0.3108839446782922\n",
      "Recall: 0.7322946175637394\n",
      "F1 Score: 0.43647108484592656\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('casehold/legalbert',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "52f2dae8794c49cb922ccb83601f9301",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.6844444444444444\n",
      "Precision: 0.652542372881356\n",
      "Recall: 0.2823960880195599\n",
      "F1 Score: 0.39419795221843\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/parallel_three_mlm_casehold',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b4a5bc6f1ef04f21b234971dfa815631",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/141 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.6728888888888889\n",
      "Precision: 0.7009803921568627\n",
      "Recall: 0.17481662591687042\n",
      "F1 Score: 0.27984344422700586\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/sentence_pair_classification',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02e096c4f9954ff489c9be3349467731",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.676\n",
      "Precision: 0.6074074074074074\n",
      "Recall: 0.23229461756373937\n",
      "F1 Score: 0.3360655737704918\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/mlm_casehold',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "64b0af9e9de74d05980a0a185c585bab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./models/casehold_mlm and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.475\n",
      "Precision: 0.39858490566037735\n",
      "Recall: 0.9575070821529745\n",
      "F1 Score: 0.5628642797668609\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/casehold_mlm',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "98597990f37b434181dba7a7a52ff65a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at ./models/mlm_sts and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.647\n",
      "Precision: 0.0\n",
      "Recall: 0.0\n",
      "F1 Score: 0.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1497: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 due to no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the the current cell or a previous cell. Please review the code in the cell(s) to identify a possible cause of the failure. Click <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. View Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/mlm_sts',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5766e75be8404637aaa5bf92872152fc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/125 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n",
      "Evaluating [Sequence Pair Classificaiton Evaluation Metrics]\n",
      "============================================\n",
      "None\n",
      "Accuracy: 0.697\n",
      "Precision: 0.6700680272108843\n",
      "Recall: 0.2790368271954674\n",
      "F1 Score: 0.394\n"
     ]
    }
   ],
   "source": [
    "evaluate_sequence_pair_class('./models/parallel_three_mlm_casehold',  'Sequence Pair Classificaiton Evaluation Metrics')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
