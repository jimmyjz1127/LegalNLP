{"cells":[{"cell_type":"markdown","id":"-3csdqPUlP8S","metadata":{"id":"-3csdqPUlP8S"},"source":["# For preparing fine-tuning data for MLM (Masked Language Modelling)"]},{"cell_type":"code","execution_count":1,"id":"gor6XhomVjw8","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8928,"status":"ok","timestamp":1705857693523,"user":{"displayName":"Jimmy Zhang","userId":"02236096453379548022"},"user_tz":0},"id":"gor6XhomVjw8","outputId":"948b9d9a-5ba5-40be-d0b7-dfca46a10373"},"outputs":[{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /home/jz75/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n"]}],"source":["import os\n","import json\n","import re\n","import string\n","import random\n","import time\n","import datetime\n","import numpy as np\n","import pandas as pd\n","from argparse import Namespace\n","\n","# from transformers import BertTokenizer\n","\n","import torch\n","import random\n","import nltk\n","nltk.download('punkt')\n","from nltk.tokenize import word_tokenize"]},{"cell_type":"markdown","id":"PVtjhTL3VhYW","metadata":{"id":"PVtjhTL3VhYW"},"source":["Prepare configurations for data preparation"]},{"cell_type":"code","execution_count":18,"id":"c485ccf9","metadata":{"id":"c485ccf9"},"outputs":[],"source":["args = Namespace(\n","    raw_data_filename = 'aus_sum_cases_train.json',\n","    raw_data_path = \"./raw_data/\",\n","    processed_data_path = \"./processed_data/\",\n","    max_segment_length = 512,\n","    max_samples = 3000,\n","    mode = 'a', # a : append, w : write,\n",")"]},{"cell_type":"markdown","id":"9f0f5c6c","metadata":{"id":"9f0f5c6c"},"source":["## Ingest Raw Data"]},{"cell_type":"markdown","id":"955c9dca","metadata":{"id":"955c9dca"},"source":["Format :\n","{\\\n","    &nbsp;&nbsp; query\\\n","    &nbsp;&nbsp; text \\\n","    &nbsp;&nbsp; topic_label\\\n","    &nbsp;&nbsp; secondary_query\\\n","    &nbsp;&nbsp; label\\\n","}"]},{"cell_type":"markdown","id":"b47913ec","metadata":{"id":"b47913ec"},"source":["### Using NLTK for tokenization"]},{"cell_type":"code","execution_count":5,"id":"dba0fd88","metadata":{},"outputs":[],"source":["def extract_sentences_json(all_sentences):\n","\n","    with open(args.raw_data_path + args.raw_data_filename, 'r') as json_file:\n","        json_data = json.load(json_file)\n","    counter = 0\n","\n","    for item in json_data:\n","        counter += 1\n","        if len(all_sentences) > args.max_samples : break\n","\n","        sentences = nltk.sent_tokenize(item['text']) + nltk.sent_tokenize(item['query'])\n","\n","        i = 0\n","        while i < len(sentences):\n","            sentence = sentences[i]\n","    #         i = i + 5 # every 5 sentences for more uniform coverage of documents in data\n","            i += 1\n","\n","            # if sentence/sample larger than 512 characters\n","            if len(sentence) > 512 or len(sentence.split(' ')) <= 10: continue\n","            # if we've reached maximum number of samples needed\n","            if len(all_sentences) >= args.max_samples: break\n","\n","            all_sentences.append(sentence)\n","\n","    print(counter)\n","    print(len(all_sentences))"]},{"cell_type":"code","execution_count":6,"id":"e8db345d","metadata":{},"outputs":[],"source":["def extract_sentences_casehold(all_sentences):\n","    casehold_df = pd.read_csv(args.processed_data_path + args.raw_data_filename)\n","    counter = 0\n","\n","    for index,row in casehold_df.iterrows():\n","        counter+=1\n","        if len(all_sentences) >= args.max_samples: break\n","\n","        sentences = nltk.sent_tokenize(row['context'])\n","\n","        i = 0\n","        while i < len(sentences):\n","            sentence = sentences[i]\n","\n","            i+=1\n","\n","            # if sentence/sample larger than 512 characters\n","            if len(sentence) > 512 or len(sentence.split(' ')) <= 10: continue\n","            # if we've reached maximum number of samples needed\n","            if len(all_sentences) >= args.max_samples: break\n","\n","            all_sentences.append(sentence)\n","    print(counter)\n","    print(len(all_sentences))"]},{"cell_type":"code","execution_count":19,"id":"085bf5e5","metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["1\n","18949\n","3000\n","[\"This Act may be cited as the ``Border Hospital Survival and Illegal \\nImmigrant Care Act''.\", 'The Congress finds as follows:\\n            (1) Immigration is a Federal responsibility.', '(2) The Immigration and Naturalization Service does not \\n        take into custody all aliens who are unlawfully present in the \\n        United States.']\n"]}],"source":["\n","all_sentences = []\n","\n","# extract sentences from data\n","if args.raw_data_filename.split('.').pop() == 'json':\n","    print('1')\n","    extract_sentences_json(all_sentences)\n","else:\n","    print('2')\n","    extract_sentences_casehold(all_sentences)\n","\n","\n","\n","print(all_sentences[0:3])"]},{"cell_type":"markdown","id":"015b7fc3","metadata":{"id":"015b7fc3"},"source":["## Process/Prepare Raw Data"]},{"cell_type":"code","execution_count":20,"id":"f294bfa0","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":260,"status":"ok","timestamp":1705857919608,"user":{"displayName":"Jimmy Zhang","userId":"02236096453379548022"},"user_tz":0},"id":"f294bfa0","outputId":"d44fccf9-b4c2-4454-ffb7-9a80eb43db75"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>This Act may be cited as the ``Border Hospital...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>The Congress finds as follows:\\n            (1...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>(2) The Immigration and Naturalization Service...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>(4) The Southwest border region is ill-equippe...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>(5) The Southwest border region has been desig...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence\n","0  This Act may be cited as the ``Border Hospital...\n","1  The Congress finds as follows:\\n            (1...\n","2  (2) The Immigration and Naturalization Service...\n","3  (4) The Southwest border region is ill-equippe...\n","4  (5) The Southwest border region has been desig..."]},"execution_count":20,"metadata":{},"output_type":"execute_result"}],"source":["df = pd.DataFrame(all_sentences, columns=['sentence'])\n","\n","df.head()"]},{"cell_type":"code","execution_count":14,"id":"ecb83ce9","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":206},"executionInfo":{"elapsed":244,"status":"ok","timestamp":1705857953179,"user":{"displayName":"Jimmy Zhang","userId":"02236096453379548022"},"user_tz":0},"id":"ecb83ce9","outputId":"ae1e60d9-db1b-4f42-b717-477305657c9c"},"outputs":[{"data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>sentence</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>0</th>\n","      <td>Drapeau’s cohorts, the cohort would be a “vict...</td>\n","    </tr>\n","    <tr>\n","      <th>1</th>\n","      <td>Felony offenses that involve explosives qualif...</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>§ 924(e)(2)(B)(ii) (defining a “violent felony...</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>Courts have found possession of a'bomb to be a...</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>See United States v. Newman, 125 F.3d 863 (10t...</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                                            sentence\n","0  Drapeau’s cohorts, the cohort would be a “vict...\n","1  Felony offenses that involve explosives qualif...\n","2  § 924(e)(2)(B)(ii) (defining a “violent felony...\n","3  Courts have found possession of a'bomb to be a...\n","4  See United States v. Newman, 125 F.3d 863 (10t..."]},"execution_count":14,"metadata":{},"output_type":"execute_result"}],"source":["df.head()"]},{"cell_type":"code","execution_count":21,"id":"c7a4617a","metadata":{"id":"c7a4617a"},"outputs":[],"source":["# Save data to file\n","df.to_csv(args.processed_data_path + '/sentences.csv', mode=args.mode)"]},{"cell_type":"code","execution_count":22,"id":"d46b1d3d","metadata":{"id":"d46b1d3d"},"outputs":[],"source":["dataframe = pd.read_csv(args.processed_data_path + '/sentences.csv')\n","\n","dataframe = dataframe.sample(frac=1).reset_index(drop=True)\n","\n","dataframe.to_csv(args.processed_data_path + '/sentences.csv', mode='w')"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
