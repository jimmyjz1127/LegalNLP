{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Application Core Functionality \n",
    "#### Finding relevant documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-24 03:09:13.592525: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-24 03:09:14.473608: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "# import plotly.io as pio\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    corpus_path=\"./processed_data/search_test.csv\",\n",
    "    model_path=\"models/combined/parallel_combined\",\n",
    "    tfidf_pkl_path=\"./TFIDF/tfidf.pkl\",\n",
    "    num_results=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>query</th>\n",
       "      <th>main</th>\n",
       "      <th>label</th>\n",
       "      <th>name</th>\n",
       "      <th>idx</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>as significantly restricted in his ability to ...</td>\n",
       "      <td>holding that an employer did not regard the em...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>will my photos be sold to facebook?</td>\n",
       "      <td>Please contact us, as provided above, to make ...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>do you sell my data</td>\n",
       "      <td>You can object to the processing of your infor...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>which permissions does the app require and wha...</td>\n",
       "      <td>If you want to stop receiving personalized ads...</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>interest rates</td>\n",
       "      <td>The rate of interest paid under the Term Note ...</td>\n",
       "      <td>1</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               query  \\\n",
       "0  as significantly restricted in his ability to ...   \n",
       "1                will my photos be sold to facebook?   \n",
       "2                                do you sell my data   \n",
       "3  which permissions does the app require and wha...   \n",
       "4                                     interest rates   \n",
       "\n",
       "                                                main  label  name  idx  \n",
       "0  holding that an employer did not regard the em...      0   NaN    0  \n",
       "1  Please contact us, as provided above, to make ...      0   NaN    1  \n",
       "2  You can object to the processing of your infor...      0   NaN    2  \n",
       "3  If you want to stop receiving personalized ads...      0   NaN    3  \n",
       "4  The rate of interest paid under the Term Note ...      1   NaN    4  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv(args.corpus_path)\n",
    "df = df[['query','main','label','name']]\n",
    "df['idx'] = df.index\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Pretrained model and tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#METHOD 1 : TF-IDF ==> BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jz75/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "class tfidf_corp:\n",
    "    '''\n",
    "        Class definition of tfidf_corp object for building TF-IDF matrix of document corpus and performing \n",
    "        cosine similarity searches.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, datapath):\n",
    "        '''\n",
    "            Constructor : initializes vectorizer object, corpus TF-IDF matrix, empty document list, and stopword list\n",
    "        '''\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.corpus_tfidf = None\n",
    "        self.documents = []\n",
    "        self.stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "        self.datapath = datapath\n",
    "\n",
    "    def set_documents(self, df):\n",
    "        self.documents = df\n",
    "\n",
    "    def load_documents(self):\n",
    "        with open(self.datapath, 'r') as corpus_file:\n",
    "            self.documents = json.load(corpus_file)\n",
    "\n",
    "    def add_document(self, document):\n",
    "        '''\n",
    "            Appends a single document objects to documents list class-attribute \n",
    "\n",
    "            Arguments:\n",
    "                document : document json object (main, name, ..., extra)\n",
    "        '''\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''\n",
    "            Appends list of documents to documents list class-attribute \n",
    "\n",
    "            Arguments:\n",
    "                documents : list of document json objects [{main, name, ..., extra}]\n",
    "        '''\n",
    "        self.documents = self.documents + documents\n",
    "    \n",
    "    def generate_tfidf(self):\n",
    "        '''\n",
    "            Computes TF-IDF matrix for document corpus \n",
    "        '''\n",
    "\n",
    "        if len(self.documents) < 1:\n",
    "            print('No documents in corpus')\n",
    "            return\n",
    "\n",
    "        self.corpus_tfidf = self.vectorizer.fit_transform([obj['main'] for idx,obj in self.documents.iterrows()])\n",
    "\n",
    "    def search(self, query, k):\n",
    "        '''\n",
    "            Performs cosine similarity search for query against document corpus \n",
    "        '''\n",
    "\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        similarities = linear_kernel(query_vector, self.corpus_tfidf).flatten()\n",
    "\n",
    "        ranked_documents = [(self.documents.loc[i], score) for i, score in enumerate(similarities) if score > 0]\n",
    "        ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return ranked_documents[0:k]\n",
    "\n",
    "\n",
    "    def store_matrix(self, path):\n",
    "        '''\n",
    "            Saves TF-IDF matrix into pickle file \n",
    "        '''\n",
    "\n",
    "        with open(path, 'wb') as pickle_file:\n",
    "            pickle.dump((self.vectorizer, self.corpus_tfidf), pickle_file)\n",
    "\n",
    "\n",
    "    def load_matrix(self, path):\n",
    "        ''' \n",
    "            Loads TF-IDF matrix from pickle file \n",
    "        '''\n",
    "\n",
    "        with open('Embeddings/tfidf.pkl', 'rb') as pickle_file:\n",
    "            self.vectorizer, self.corpus_tfidf = pickle.load(pickle_file) # need to save both vectorizer object and matrix to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = tfidf_corp(args.corpus_path)\n",
    "\n",
    "# engine.load_documents()\n",
    "engine.set_documents(df)\n",
    "\n",
    "engine.generate_tfidf()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search1(query, df, model_path):\n",
    "    # engine = tfidf_corp(args.corpus_path)\n",
    "\n",
    "    # engine.load_documents()\n",
    "    # engine.set_documents(df)\n",
    "\n",
    "    # engine.generate_tfidf()\n",
    "\n",
    "    # engine.store_matrix(args.tfidf_pkl_path)\n",
    "\n",
    "    top_k_tfidf = engine.search(query, 100)\n",
    "\n",
    "\n",
    "    df_rows = [row for row,_ in top_k_tfidf]\n",
    "\n",
    "    dataframe = pd.concat(df_rows, axis=1).transpose()\n",
    "\n",
    "    dataframe.head()\n",
    "\n",
    "    model = BertForSequenceClassification.from_pretrained(model_path)\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('casehold/legalbert')\n",
    "\n",
    "    device = \"cpu\"\n",
    " \n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "\n",
    "    model.to(device)\n",
    "    \n",
    "    similarity_scores = []\n",
    "\n",
    "    for main_text in dataframe['main']:\n",
    "        combined_input = query + \" [SEP] \" + main_text\n",
    "\n",
    "        # Tokenize and encode the text for the model input\n",
    "        text_tokens = tokenizer(combined_input, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        text_tokens = {key: value.to(device) for key, value in text_tokens.items()}\n",
    "        \n",
    "        # Get text embedding\n",
    "        with torch.no_grad():\n",
    "            model_output = model(**text_tokens)\n",
    "            logits = model_output.logits \n",
    "            score = torch.nn.functional.softmax(logits, dim=1)[:,1].item()\n",
    "            similarity_scores.append(score)\n",
    "        \n",
    "        \n",
    "\n",
    "    # Add similarity scores to the dataframe\n",
    "    dataframe['similarity'] = similarity_scores\n",
    "    \n",
    "    # Sort the dataframe by similarity scores in descending order\n",
    "    sorted_dataframe = dataframe.sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # Optionally, you might want to drop the similarity column before returning\n",
    "    # sorted_dataframe.drop(columns=['similarity'], inplace=True)\n",
    "    \n",
    "    return sorted_dataframe.iloc[0]['idx']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "query    is there 2-step verification in case somebody ...\n",
       "main     If you use Evernote Business, the Account Hold...\n",
       "label                                                    0\n",
       "name                                                   NaN\n",
       "idx                                                     14\n",
       "Name: 14, dtype: object"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.iloc[14]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.021627188465499485\n"
     ]
    }
   ],
   "source": [
    "num_correct = 0\n",
    "total = 0\n",
    "for idx, row in df.iterrows():\n",
    "    try:\n",
    "        result = search1(row['query'], df, args.model_path)\n",
    "\n",
    "        if row['idx'] == result : num_correct += 1\n",
    "        total += 1\n",
    "    except (ValueError ): continue\n",
    "\n",
    "\n",
    "print(num_correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
