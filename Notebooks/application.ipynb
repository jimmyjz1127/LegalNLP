{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Search Engine Application Core Functionality \n",
    "#### Finding relevant documents "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jz75/Documents/2023-2024/SH-Project/CS4099-LegalNLP/pytorchenv/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import json\n",
    "import re\n",
    "import string\n",
    "import random\n",
    "import time\n",
    "import datetime\n",
    "import copy\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "# import plotly.express as px\n",
    "from tqdm import tqdm\n",
    "# import plotly.io as pio\n",
    "\n",
    "from argparse import Namespace\n",
    "from tqdm import tqdm\n",
    "from datasets import Dataset\n",
    "\n",
    "import transformers\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
    "from transformers import BertForSequenceClassification, AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from transformers import pipeline\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer,CountVectorizer\n",
    "from sklearn.feature_extraction import text\n",
    "from sklearn.metrics.pairwise import cosine_similarity, linear_kernel\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, SnowballStemmer\n",
    "\n",
    "import pickle\n",
    "\n",
    "import faiss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "args = Namespace(\n",
    "    corpus_path=\"./../Dev/corpus.json\",\n",
    "    model_path=\"models/mlm_model_manual\",\n",
    "    tfidf_pkl_path=\"./TFIDF/tfidf.pkl\",\n",
    "    num_results=10\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>main</th>\n",
       "      <th>date</th>\n",
       "      <th>jurisdictions</th>\n",
       "      <th>court</th>\n",
       "      <th>attorneys</th>\n",
       "      <th>extra</th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Ann M. Osborn and Thomas Osborn, Plaintiffs in...</td>\n",
       "      <td>Ann M. Osborn and Thomas Osborn, Plaintiffs in...</td>\n",
       "      <td>1857-12-01</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Illinois Supreme Court</td>\n",
       "      <td>[Underwoods, for Plaintiffs in Error., G. Koer...</td>\n",
       "      <td>[{'text': 'Catón, C. J.\n",
       "Mrs. Osborn was entitl...</td>\n",
       "      <td>AnnMOsbornandThomasOsbornPlaintiffsinErrorvJac...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Nathan Prentice, Appellant, v. Phineas Kimball...</td>\n",
       "      <td>Nathan Prentice, Appellant, v. Phineas Kimball...</td>\n",
       "      <td>1857-12-01</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Illinois Supreme Court</td>\n",
       "      <td>[C. L. Higbee, for Appellant., J. Grimshaw and...</td>\n",
       "      <td>[{'text': 'Breese, J.\n",
       "It is a rule in courts o...</td>\n",
       "      <td>NathanPrenticeAppellantvPhineasKimballAppellee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Edward Haven et al., Plaintiffs in Error, v. H...</td>\n",
       "      <td>Edward Haven et al., Plaintiffs in Error, v. H...</td>\n",
       "      <td>1857-12-01</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Illinois Supreme Court</td>\n",
       "      <td>[G. Koerner, for Plaintiffs in Error., G. Trum...</td>\n",
       "      <td>[{'text': 'Breese, J.\n",
       "On the 12th day of Eebru...</td>\n",
       "      <td>EdwardHavenetalPlaintiffsinErrorvHilmanMehlgar...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Stephen R. Rowan and Nancy Ann, his Wife, Comp...</td>\n",
       "      <td>Stephen R. Rowan and Nancy Ann, his Wife, Comp...</td>\n",
       "      <td>1857-11-01</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Illinois Supreme Court</td>\n",
       "      <td>[Nelson &amp; Johnson, for Appellants., J. Olney a...</td>\n",
       "      <td>[{'text': 'Breese, Justice,\n",
       "delivered the opin...</td>\n",
       "      <td>StephenRRowanandNancyAnnhisWifeComplainantsApp...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Thomas Rodney, Plaintiff in Error, v. The Illi...</td>\n",
       "      <td>Thomas Rodney, Plaintiff in Error, v. The Illi...</td>\n",
       "      <td>1857-11-01</td>\n",
       "      <td>Illinois</td>\n",
       "      <td>Illinois Supreme Court</td>\n",
       "      <td>[J. Dougherty, for Plaintiff in Error., C. G. ...</td>\n",
       "      <td>[{'text': 'Skinner, J.\n",
       "The plaintiff sued the ...</td>\n",
       "      <td>ThomasRodneyPlaintiffinErrorvTheIllinoisCentra...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                name  \\\n",
       "0  Ann M. Osborn and Thomas Osborn, Plaintiffs in...   \n",
       "1  Nathan Prentice, Appellant, v. Phineas Kimball...   \n",
       "2  Edward Haven et al., Plaintiffs in Error, v. H...   \n",
       "3  Stephen R. Rowan and Nancy Ann, his Wife, Comp...   \n",
       "4  Thomas Rodney, Plaintiff in Error, v. The Illi...   \n",
       "\n",
       "                                                main       date jurisdictions  \\\n",
       "0  Ann M. Osborn and Thomas Osborn, Plaintiffs in... 1857-12-01      Illinois   \n",
       "1  Nathan Prentice, Appellant, v. Phineas Kimball... 1857-12-01      Illinois   \n",
       "2  Edward Haven et al., Plaintiffs in Error, v. H... 1857-12-01      Illinois   \n",
       "3  Stephen R. Rowan and Nancy Ann, his Wife, Comp... 1857-11-01      Illinois   \n",
       "4  Thomas Rodney, Plaintiff in Error, v. The Illi... 1857-11-01      Illinois   \n",
       "\n",
       "                    court                                          attorneys  \\\n",
       "0  Illinois Supreme Court  [Underwoods, for Plaintiffs in Error., G. Koer...   \n",
       "1  Illinois Supreme Court  [C. L. Higbee, for Appellant., J. Grimshaw and...   \n",
       "2  Illinois Supreme Court  [G. Koerner, for Plaintiffs in Error., G. Trum...   \n",
       "3  Illinois Supreme Court  [Nelson & Johnson, for Appellants., J. Olney a...   \n",
       "4  Illinois Supreme Court  [J. Dougherty, for Plaintiff in Error., C. G. ...   \n",
       "\n",
       "                                               extra  \\\n",
       "0  [{'text': 'Catón, C. J.\n",
       "Mrs. Osborn was entitl...   \n",
       "1  [{'text': 'Breese, J.\n",
       "It is a rule in courts o...   \n",
       "2  [{'text': 'Breese, J.\n",
       "On the 12th day of Eebru...   \n",
       "3  [{'text': 'Breese, Justice,\n",
       "delivered the opin...   \n",
       "4  [{'text': 'Skinner, J.\n",
       "The plaintiff sued the ...   \n",
       "\n",
       "                                                  id  \n",
       "0  AnnMOsbornandThomasOsbornPlaintiffsinErrorvJac...  \n",
       "1     NathanPrenticeAppellantvPhineasKimballAppellee  \n",
       "2  EdwardHavenetalPlaintiffsinErrorvHilmanMehlgar...  \n",
       "3  StephenRRowanandNancyAnnhisWifeComplainantsApp...  \n",
       "4  ThomasRodneyPlaintiffinErrorvTheIllinoisCentra...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_json(args.corpus_path)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up Pretrained model and tokeniser"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#METHOD 1 : TF-IDF ==> BERT "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/jz75/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "class tfidf_corp:\n",
    "    '''\n",
    "        Class definition of tfidf_corp object for building TF-IDF matrix of document corpus and performing \n",
    "        cosine similarity searches.\n",
    "    '''\n",
    "\n",
    "\n",
    "    def __init__(self, datapath):\n",
    "        '''\n",
    "            Constructor : initializes vectorizer object, corpus TF-IDF matrix, empty document list, and stopword list\n",
    "        '''\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.corpus_tfidf = None\n",
    "        self.documents = []\n",
    "        self.stop_words = set(stopwords.words('english') + list(string.punctuation))\n",
    "        self.datapath = datapath\n",
    "\n",
    "    def set_documents(self, df):\n",
    "        self.documents = df\n",
    "\n",
    "    def load_documents(self):\n",
    "        with open(self.datapath, 'r') as corpus_file:\n",
    "            self.documents = json.load(corpus_file)\n",
    "\n",
    "    def add_document(self, document):\n",
    "        '''\n",
    "            Appends a single document objects to documents list class-attribute \n",
    "\n",
    "            Arguments:\n",
    "                document : document json object (main, name, ..., extra)\n",
    "        '''\n",
    "        self.documents.append(document)\n",
    "\n",
    "    def add_documents(self, documents):\n",
    "        '''\n",
    "            Appends list of documents to documents list class-attribute \n",
    "\n",
    "            Arguments:\n",
    "                documents : list of document json objects [{main, name, ..., extra}]\n",
    "        '''\n",
    "        self.documents = self.documents + documents\n",
    "    \n",
    "    def generate_tfidf(self):\n",
    "        '''\n",
    "            Computes TF-IDF matrix for document corpus \n",
    "        '''\n",
    "\n",
    "        if len(self.documents) < 1:\n",
    "            print('No documents in corpus')\n",
    "            return\n",
    "\n",
    "        self.corpus_tfidf = self.vectorizer.fit_transform([obj['main'] for idx,obj in self.documents.iterrows()])\n",
    "\n",
    "    def search(self, query, k):\n",
    "        '''\n",
    "            Performs cosine similarity search for query against document corpus \n",
    "        '''\n",
    "\n",
    "        query_vector = self.vectorizer.transform([query])\n",
    "        similarities = linear_kernel(query_vector, self.corpus_tfidf).flatten()\n",
    "\n",
    "        ranked_documents = [(self.documents.loc[i], score) for i, score in enumerate(similarities) if score > 0]\n",
    "        ranked_documents.sort(key=lambda x: x[1], reverse=True)\n",
    "\n",
    "        return ranked_documents[0:k]\n",
    "\n",
    "\n",
    "    def store_matrix(self, path):\n",
    "        '''\n",
    "            Saves TF-IDF matrix into pickle file \n",
    "        '''\n",
    "\n",
    "        with open(path, 'wb') as pickle_file:\n",
    "            pickle.dump((self.vectorizer, self.corpus_tfidf), pickle_file)\n",
    "\n",
    "\n",
    "    def load_matrix(self, path):\n",
    "        ''' \n",
    "            Loads TF-IDF matrix from pickle file \n",
    "        '''\n",
    "\n",
    "        with open('Embeddings/tfidf.pkl', 'rb') as pickle_file:\n",
    "            self.vectorizer, self.corpus_tfidf = pickle.load(pickle_file) # need to save both vectorizer object and matrix to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def search1(query, df, model_path):\n",
    "    engine = tfidf_corp(args.corpus_path)\n",
    "\n",
    "    # engine.load_documents()\n",
    "    engine.set_documents(df)\n",
    "\n",
    "    engine.generate_tfidf()\n",
    "\n",
    "    # engine.store_matrix(args.tfidf_pkl_path)\n",
    "\n",
    "    top_k_tfidf = engine.search(query, 10)\n",
    "\n",
    "\n",
    "    df_rows = [row for row,_ in top_k_tfidf]\n",
    "\n",
    "    dataframe = pd.concat(df_rows, axis=1).transpose()\n",
    "\n",
    "\n",
    "    model = transformers.BertModel.from_pretrained(model_path)\n",
    "    tokenizer = transformers.BertTokenizer.from_pretrained('casehold/legalbert')\n",
    "\n",
    "    device = \"cpu\"\n",
    " \n",
    "    if torch.cuda.is_available():\n",
    "        device = 'cuda'\n",
    "\n",
    "    model.to(device)\n",
    "\n",
    "    query_tokens = tokenizer(query, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "    query_tokens = {key: value.to(device) for key, value in query_tokens.items()}\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        query_embedding = model(**query_tokens).last_hidden_state.mean(dim=1)\n",
    "\n",
    "    similarity_scores = []\n",
    "\n",
    "    for main_text in dataframe['main']:\n",
    "\n",
    "        # Tokenize and encode the text for the model input\n",
    "        text_tokens = tokenizer(main_text, return_tensors='pt', padding=True, truncation=True, max_length=512)\n",
    "        text_tokens = {key: value.to(device) for key, value in text_tokens.items()}\n",
    "        \n",
    "        # Get text embedding\n",
    "        with torch.no_grad():\n",
    "            text_embedding = model(**text_tokens).last_hidden_state.mean(dim=1)\n",
    "        \n",
    "        # Compute cosine similarity and append to list\n",
    "        similarity = cosine_similarity(query_embedding.cpu().numpy(), text_embedding.cpu().numpy())[0][0]\n",
    "        similarity_scores.append(similarity)\n",
    "\n",
    "    # Add similarity scores to the dataframe\n",
    "    dataframe['similarity'] = similarity_scores\n",
    "    \n",
    "    # Sort the dataframe by similarity scores in descending order\n",
    "    sorted_dataframe = dataframe.sort_values(by='similarity', ascending=False)\n",
    "    \n",
    "    # Optionally, you might want to drop the similarity column before returning\n",
    "    # sorted_dataframe.drop(columns=['similarity'], inplace=True)\n",
    "    \n",
    "    return sorted_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at models/mlm_model_manual and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>similarity</th>\n",
       "      <th>name</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>281</th>\n",
       "      <td>0.512436</td>\n",
       "      <td>The Illinois Central Railroad Co. v. Carl Wodr...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166</th>\n",
       "      <td>0.460161</td>\n",
       "      <td>The Butler Street Foundry and Iron Company, De...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>417</th>\n",
       "      <td>0.446165</td>\n",
       "      <td>The Illinois Educational Association v. Peter ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>0.385628</td>\n",
       "      <td>Charles H. Austin et al., by their next friend...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>0.358708</td>\n",
       "      <td>Charles Sprague, Plaintiff in Error, v. The Il...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.345513</td>\n",
       "      <td>Thomas Rodney, Plaintiff in Error, v. The Illi...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     similarity                                               name\n",
       "281    0.512436  The Illinois Central Railroad Co. v. Carl Wodr...\n",
       "166    0.460161  The Butler Street Foundry and Iron Company, De...\n",
       "417    0.446165  The Illinois Educational Association v. Peter ...\n",
       "89     0.385628  Charles H. Austin et al., by their next friend...\n",
       "84     0.358708  Charles Sprague, Plaintiff in Error, v. The Il...\n",
       "4      0.345513  Thomas Rodney, Plaintiff in Error, v. The Illi..."
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result = search1('illinois defendent', df, args.model_path)\n",
    "result.loc[:4, ['similarity', 'name']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# METHOD 2 : FAISS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embed_sentence(model, tokenizer, text):\n",
    "    '''\n",
    "        For embedding a given sentence using the pre-trained BERT model\n",
    "    '''\n",
    "\n",
    "    with torch.no_grad():\n",
    "        tokens = tokenizer.encode(text)\n",
    "        batch_tokens = np.expand_dims(tokens, axis=0)\n",
    "        batch_tokens = torch.tensor(batch_tokens).cuda()\n",
    "        return model(batch_tokens)[0].cpu()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_embedding_mean(embedding):\n",
    "    if not isinstance(embedding, torch.Tensor):\n",
    "        print('Embedding must be a torch.Tensor')\n",
    "        return\n",
    "    return embedding.mean(1)\n",
    "\n",
    "def compute_cosine_measure(x1, x2):\n",
    "    return cosine_similarity(x1,x2)\n",
    "\n",
    "def compute_distance(x1, x2):\n",
    "    return compute_cosine_measure(x1.detach().numpy(), x2.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHUNK_SIZE_EACH =50\n",
    "\n",
    "\n",
    "\n",
    "def __embedding(text, model, tokenizer):\n",
    "  return compute_embedding_mean(embed_sentence(model,tokenizer, text))\n",
    "\n",
    "\n",
    "\n",
    "def compute_bert_embeddings(dataframe_chunk, current_index, end_marker, model, tokenizer):\n",
    "\n",
    "  np_chunk = __embedding(dataframe_chunk.loc[current_index * end_marker]['name'], model, tokenizer).detach().numpy()\n",
    "  # np_chunk = np_chunk.reshape(np_chunk.shape[1])\n",
    "  print(end_marker)\n",
    "\n",
    "  for idx in range(1, end_marker):\n",
    "\n",
    "    try:\n",
    "      embedding = __embedding(dataframe_chunk.loc[(current_index * end_marker) + idx]['name'], model, tokenizer).detach().numpy()\n",
    "      #embedding = embedding.reshape(embedding.shape[1])\n",
    "      np_chunk = np.append(np_chunk, embedding, axis = 0)\n",
    "      # print('\\r {}'.format(np_chunk.shape), end = '')\n",
    "    except Exception as e:\n",
    "      # print(e)\n",
    "      np_chunk = np.append(np_chunk, np.zeros(shape = (1, 768)), axis = 0)\n",
    "      continue \n",
    "\n",
    "  # print(np_chunk.shape)\n",
    "  np.savez_compressed('title_{}'.format(current_index), a = np_chunk)\n",
    "\n",
    "\n",
    "def compute_embeddings_and_save(dataframe, model, tokenizer):\n",
    "\n",
    "  n_rows = len(dataframe)\n",
    "  \n",
    "  chunk_sizes = n_rows // CHUNK_SIZE_EACH\n",
    "  remaining = n_rows - chunk_sizes * CHUNK_SIZE_EACH\n",
    "\n",
    "  for i in range(0, 1):\n",
    "    print('test')\n",
    "\n",
    "    compute_bert_embeddings(dataframe[i * CHUNK_SIZE_EACH : (i * CHUNK_SIZE_EACH) + CHUNK_SIZE_EACH ], i, CHUNK_SIZE_EACH, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at models/mlm_model_manual and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "BertModel(\n",
       "  (embeddings): BertEmbeddings(\n",
       "    (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "    (position_embeddings): Embedding(512, 768)\n",
       "    (token_type_embeddings): Embedding(2, 768)\n",
       "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (encoder): BertEncoder(\n",
       "    (layer): ModuleList(\n",
       "      (0-11): 12 x BertLayer(\n",
       "        (attention): BertAttention(\n",
       "          (self): BertSelfAttention(\n",
       "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (output): BertSelfOutput(\n",
       "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (intermediate): BertIntermediate(\n",
       "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          (intermediate_act_fn): GELUActivation()\n",
       "        )\n",
       "        (output): BertOutput(\n",
       "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "          (dropout): Dropout(p=0.1, inplace=False)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "  )\n",
       "  (pooler): BertPooler(\n",
       "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "    (activation): Tanh()\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = transformers.BertModel.from_pretrained(args.model_path)\n",
    "tokenizer = transformers.BertTokenizer.from_pretrained('casehold/legalbert')\n",
    "\n",
    "device = \"cpu\"\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    device = 'cuda'\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test\n",
      "50\n"
     ]
    }
   ],
   "source": [
    "bert_embeddings = compute_embeddings_and_save(df, model, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def do_faiss_lookup(fastIndex, query_text, model, tokenizer, top_k):\n",
    "    embedding_q = compute_embedding_mean(embed_sentence(model, tokenizer, query_text)).detach().numpy()\n",
    "    \n",
    "    #let it be float32\n",
    "    embedding_q = embedding_q.astype('float32')\n",
    "    \n",
    "    #perform the search\n",
    "    st = time.time()\n",
    "    matched_em, matched_indexes = fastIndex.search(embedding_q, top_k) # it returns matched vectors and thier respective indexes, we are interested only in indexes.\n",
    "    \n",
    "    #indexes are already sorted wrt to closest match\n",
    "    et = time.time()\n",
    "    \n",
    "    return et - st, matched_indexes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "index_map = {} \n",
    "for index,row in df.iterrows():\n",
    "    index_map[index] = {\n",
    "        \"Title\" : row['name'],\n",
    "        'Body' : row['main']\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768\n",
      "Look up time :  3.719329833984375e-05 seconds\n",
      "[32 36 41 13  7 42  1 18 48 33]\n",
      "0. 32    The Nokomis Coal Company, Plaintiff in Error, ...\n",
      "36    The Peabody Coal Company, Plaintiff in Error, ...\n",
      "41    Frederick Brown, Plaintiff in Error, v. The Pe...\n",
      "13    Daniel Finch et al., Apellants, v. Emma C. Mar...\n",
      "7     Isaac C. Choate, Plaintiff in Error, v. The Pe...\n",
      "42    Jacob H. Detrick, Appellant, v. Eli Migatt et ...\n",
      "1     Nathan Prentice, Appellant, v. Phineas Kimball...\n",
      "18    Richard A. Gregory and Wife, Appellants, v. Ly...\n",
      "48    George Forquer et al., Appellants, v. Susannah...\n",
      "33    The Groveland Coal Mining Company, Plaintiff i...\n",
      "Name: name, dtype: object\n"
     ]
    }
   ],
   "source": [
    "\n",
    "bert_embeddings = np.load('title_0.npz')['a']\n",
    "n_dimensions = bert_embeddings.shape[1] #Number of dimensions (764)\n",
    "\n",
    "print(n_dimensions)\n",
    "\n",
    "# We will create an index of type FlatL2, there are many kinds of indexes, you can look at it in their \n",
    "fastIndex = faiss.IndexFlatL2(n_dimensions) \n",
    "\n",
    "# Add the embedding vector to faiss index, it should of dtype 'float32'\n",
    "fastIndex.add(bert_embeddings.astype('float32'))\n",
    "\n",
    "time_faiss_cpu, indexes_top_faiss = do_faiss_lookup(fastIndex, 'illinois defendent', model, tokenizer, 10)\n",
    "\n",
    "print('Look up time : ', time_faiss_cpu, 'seconds')\n",
    "\n",
    "\n",
    "print(indexes_top_faiss[0])\n",
    "\n",
    "for i,idx in enumerate(indexes_top_faiss):\n",
    "    print('{}. {}'.format(i, df.loc[idx, 'name']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorchenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
